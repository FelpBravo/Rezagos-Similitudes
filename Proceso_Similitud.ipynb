{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dataflow.magics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "command = {\n",
    "    \"compartmentId\": \"ocid1.compartment.oc1..aaaaaaaaxex562zeky4c3pdibw4umq3irexcnf4qrho3fnvjbybl3nyd56wq\",\n",
    "    \"displayName\": \"Proceso Rezago Similitud 3.0\",\n",
    "    \"language\": \"PYTHON\",\n",
    "    \"sparkVersion\": \"3.2.1\",\n",
    "    \"driverShape\": \"VM.Standard.E4.Flex\",\n",
    "    \"executorShape\": \"VM.Standard.E4.Flex\",\n",
    "    \"driverShapeConfig\":{\"ocpus\":4,\"memoryInGBs\":32},   #4-32\n",
    "    \"executorShapeConfig\":{\"ocpus\":16,\"memoryInGBs\":128}, #16-128\n",
    "    \"numExecutors\": 32, #10  #15 executors\n",
    "    \"type\": \"SESSION\",\n",
    "    \"logsBucketUri\": \"oci://dataflow-logs@ax1trrrsni7e/\",\n",
    "    \"configuration\": {\"spark.archives\":\"oci://bucket-test1@ax1trrrsni7e/conda_environments/cpu/pyspark32_p38_cpu_v3/2.0/pyspark32_p38_cpu_v3v2_0#conda\",\"spark.driver.maxResultSize\":\"100g\"}# 200GB nextstep\n",
    "\n",
    "}\n",
    "command = f'\\'{json.dumps(command)}\\''\n",
    " \n",
    "%create_session -l python -c $command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F \n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import time\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from datetime import datetime\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "##        Carga Rezagos y Afiliados(MAC) Limpios Desde DataIntegration - Solo Columnas Necesarias             ##\n",
    "################################################################################################################\n",
    "file_name_rezago = 'previred_rezagos_files/tmp/rezagos_input.parquet'\n",
    "file_name_afiliado = 'previred_rezagos_files/tmp/afiliados_input.parquet'\n",
    "\n",
    "start = time.time()\n",
    "input_rezagos_master = spark.read.parquet(f\"oci://previred_rezagos_files@ax1trrrsni7e/{file_name_rezago}\")\n",
    "input_rezagos_master = input_rezagos_master.drop_duplicates().limit(5)    #...Evaluar si es necesario.\n",
    "end = time.time()\n",
    "print('Tiempo de Carga de Rezagos               : ',end - start, 'Segundos. \\n', 'Cantidad: ', str(input_rezagos_master.count()), '\\n')\n",
    "\n",
    "start = time.time()\n",
    "input_afiliados_master = spark.read.parquet(f\"oci://previred_rezagos_files@ax1trrrsni7e/{file_name_afiliado}\")\n",
    "input_afiliados_master = input_afiliados_master.drop_duplicates()#.limit(1) #...Evaluar si es necesario.\n",
    "end = time.time()\n",
    "print('Tiempo de Carga de Afiliados             : ',end - start , 'Segundos. \\n', 'Cantidad: ', str(input_afiliados_master.count()), '\\n')\n",
    "################################################################################################################\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "##Carga Rezagos y Afiliados(MAC) Limpios Desde DataIntegration - Para Completitud de Columnas en salida final ##\n",
    "################################################################################################################\n",
    "\n",
    "#file_paths_tmp_rzg  = [f\"oci://previred_rezagos_files@ax1trrrsni7e/previred_rezagos_files/tmp/tmp_db_rzg\"]\n",
    "file_paths_tmp_afil = [f\"oci://previred_rezagos_files@ax1trrrsni7e/previred_rezagos_files/tmp/tmp_db_afil\"]\n",
    "\n",
    "start    = time.time()\n",
    "df_tmp_rezago = spark.read.csv(\"oci://previred_rezagos_files@ax1trrrsni7e/previred_rezagos_files/tmp/rezagos_actualizada.csv\", sep=',', header=True)\n",
    "df_tmp_rezago = df_tmp_rezago.drop_duplicates()     #.limit(1) #...Evaluar si es necesario.\n",
    "end = time.time()   \n",
    "print('Tiempo de Carga de RezagosMaster   : ',end - start , 'Segundos. \\n', 'Cantidad: ', str(df_tmp_rezago.count()), '\\n')\n",
    "\n",
    "start    = time.time()\n",
    "dfs_afil = [spark.read.parquet(file_afil) for file_afil in file_paths_tmp_afil]\n",
    "if len(dfs_afil) > 1:\n",
    "    df_tmp_afiliado = dfs_afil[0]\n",
    "    df_tmp_afiliado = [df_tmp_afiliado.unionAll(d) for d in dfs_afil[1:]][0]\n",
    "else:\n",
    "    df_tmp_afiliado = dfs_afil[0]\n",
    "end = time.time()\n",
    "print('Tiempo de Carga y Unión de AfiliadosTmp : ', end - start , 'Segundos. \\n', 'Cantidad: ', str(df_tmp_afiliado.count()), '\\n')\n",
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "################################################################################################################\n",
    "##            Similitud [Rezagos VS Afiliados(MAC)] Por Dígito Verificador y Guardado en Bucket               ##\n",
    "################################################################################################################\n",
    "print('Iniciando Calculo De Similitudes')\n",
    "start_master = time.time()\n",
    "df_tmp_rezago = df_tmp_rezago.repartition(50)\n",
    "digito_verificador = ['0','1','2','3','4','5','6','7','8','9','K']\n",
    "\n",
    "\n",
    "for var in digito_verificador:\n",
    "    print(str(datetime.now().time()) + ' - Procesando Similitud con digito verificador  ' + str(var))\n",
    "\n",
    "    df_b = input_afiliados_master.filter(F.col('DV') == var)\n",
    "    df_b = df_b.fillna('')\n",
    "    df_b = df_b.repartition(50)\n",
    "    \n",
    "    #Dataframe df_tmp_afiliado Añadido en un dataframe con el universo mas disminuido. (TABLA UTILIZADA SOLO PARA COMPLETITUD DE COLUMNAS)\n",
    "    df_c = df_tmp_afiliado.filter(F.col('DV') == var)\n",
    "    df_c = df_c.fillna('')\n",
    "    df_c = df_c.repartition(50)\n",
    "    \n",
    "    df_a_1 = df_a.select('ID','RUT_AFILIADO','NOM_TRABAJADOR','APELLIDO_PATERNO','APELLIDO_MATERNO','NOM_COMPLETO','NOM_SIN_AM','DV_AFILIADO')\n",
    "    df_b_1 = df_b.select('RUT','NOMBRES','AP_PATERNO','AP_MATERNO','NOM_COMPLETO_F','NOM_SIN_AM_F','DV') \n",
    "    \n",
    "    df1_x_df2 = df_a_1.crossJoin(df_b_1)\n",
    "    \n",
    "    #...Similitud concatenadas (LEVENSHTEIN)...\n",
    "    #...df = df1_x_df2.withColumn('SIMILITUD', F.round(100 - (F.levenshtein(F.col(\"NOM_COMPLETO\"), F.col(\"NOM_COMPLETO_F\"))*100) / F.greatest(F.length(F.col(\"NOM_COMPLETO\")), F.length(F.col(\"NOM_COMPLETO_F\"))), 0).cast(IntegerType()))\n",
    "    #...df = df.withColumn('SIMILITUD_SIN_APMA', F.round(100 - (F.levenshtein(F.col(\"NOM_SIN_AM\"), F.col(\"NOM_SIN_AM_F\"))*100) / F.greatest(F.length(F.col(\"NOM_SIN_AM\")), F.length(F.col(\"NOM_SIN_AM_F\"))), 0).cast(IntegerType()))\n",
    "    \n",
    "    #...Similitud concatenadas (FUZZY WUZZY)\n",
    "    df = df1_x_df2.withColumn('SIMILITUD', (F.udf(fuzz.ratio)(F.col(\"NOM_COMPLETO\"), F.col(\"NOM_COMPLETO_F\"))))\n",
    "    df = df.withColumn('SIMILITUD_SIN_APMA', (F.udf(fuzz.ratio)(F.col(\"NOM_SIN_AM\"), F.col(\"NOM_SIN_AM_F\"))))\n",
    "    print('\\tSimilitud concatenadas     [COMPLETADA]')\n",
    "\n",
    "    #...Encontrar primer nombre del rezago.\n",
    "    df = df.withColumn('PRIMER_NOM_TRABAJADOR', F.split(F.col('NOM_TRABAJADOR'), ' ')[0] )\n",
    "    df = df.withColumn('EXISTENCIA', F.when(F.udf(fuzz.token_set_ratio)(F.col('PRIMER_NOM_TRABAJADOR'), F.col('NOMBRES')) >= 100, 1 ).otherwise(0))\n",
    "    print('\\tExistencia                 [COMPLETADA]')\n",
    "\n",
    "    #...Similitudes de Nombres\n",
    "    df = df.withColumn('SIMILITUDNOMBRE', F.round(100 - (F.levenshtein(F.col(\"NOM_TRABAJADOR\"), F.col(\"NOMBRES\"))*100) / F.greatest(F.length(F.col(\"NOMBRES\")), F.length(F.col(\"NOM_TRABAJADOR\"))), 0).cast(IntegerType()))\n",
    "    df = df.withColumn('SIMILITUDNOMBREREZAGOAPP', F.round(100 - (F.levenshtein(F.col(\"NOMBRES\"), F.col(\"APELLIDO_PATERNO\"))*100) / F.greatest(F.length(F.col(\"NOMBRES\")), F.length(F.col(\"APELLIDO_PATERNO\"))), 0).cast(IntegerType()))\n",
    "    df = df.withColumn('SIMILITUDNOMBREREZAGOAPM', F.round(100 - (F.levenshtein(F.col(\"NOMBRES\"), F.col(\"APELLIDO_MATERNO\"))*100) / F.greatest(F.length(F.col(\"NOMBRES\")), F.length(F.col(\"APELLIDO_MATERNO\"))), 0).cast(IntegerType()))\n",
    "    print('\\tSimilitud Nombres          [COMPLETADA]')\n",
    "\n",
    "    #Similitud de apellido paterno\n",
    "    df = df.withColumn('SIMILITUDAPEPATERNO', F.round(100 - (F.levenshtein(F.col(\"AP_PATERNO\"), F.col(\"APELLIDO_PATERNO\"))*100) / F.greatest(F.length(F.col(\"AP_PATERNO\")), F.length(F.col(\"APELLIDO_PATERNO\"))), 0).cast(IntegerType()))\n",
    "    df = df.withColumn('SIMILITUDAPPATREZAGONOMBRE', F.round(100 - (F.levenshtein(F.col(\"AP_PATERNO\"), F.col(\"NOM_TRABAJADOR\"))*100) / F.greatest(F.length(F.col(\"AP_PATERNO\")), F.length(F.col(\"NOM_TRABAJADOR\"))), 0).cast(IntegerType()))\n",
    "    df = df.withColumn('SIMILITUDAPPATREZAGOAPMAT', F.round(100 - (F.levenshtein(F.col(\"AP_PATERNO\"), F.col(\"APELLIDO_MATERNO\"))*100) / F.greatest(F.length(F.col(\"AP_PATERNO\")), F.length(F.col(\"APELLIDO_MATERNO\"))), 0).cast(IntegerType()))\n",
    "    print('\\tSimilitud apellido Paterno [COMPLETADA]') \n",
    "\n",
    "    #Similitud de apellido materno\n",
    "    df = df.withColumn(\n",
    "        'SIMILITUDAPEMATERNO', \n",
    "        F.when(F.isnull(F.round(100 - (F.levenshtein(F.col(\"AP_MATERNO\"), F.col(\"APELLIDO_MATERNO\"))*100) / F.greatest(F.length(F.col(\"AP_MATERNO\")), F.length(F.col(\"APELLIDO_MATERNO\"))), 0).cast(IntegerType())), F.lit(0))\n",
    "        .otherwise(F.round(100 - (F.levenshtein(F.col(\"AP_MATERNO\"), F.col(\"APELLIDO_MATERNO\"))*100) / F.greatest(F.length(F.col(\"AP_MATERNO\")), F.length(F.col(\"APELLIDO_MATERNO\"))), 0).cast(IntegerType())))\n",
    "    df = df.withColumn('SIMILITUDAPMATREZAGONOMBRE', F.round(100 - (F.levenshtein(F.col(\"AP_MATERNO\"), F.col(\"NOM_TRABAJADOR\"))*100) / F.greatest(F.length(F.col(\"AP_MATERNO\")), F.length(F.col(\"NOM_TRABAJADOR\"))), 0).cast(IntegerType()))\n",
    "    df = df.withColumn('SIMILITUDAPMATREZAGOAPPAT', F.round(100 - (F.levenshtein(F.col(\"AP_MATERNO\"), F.col(\"APELLIDO_PATERNO\"))*100) / F.greatest(F.length(F.col(\"AP_MATERNO\")), F.length(F.col(\"APELLIDO_PATERNO\"))), 0).cast(IntegerType()))\n",
    "    print('\\tSimilitud apellido Materno [COMPLETADA]') \n",
    "\n",
    "    #Similitud de rut \n",
    "    df = df.withColumn('SIMILITUD_RUT_AFILIADO', F.round(100 - (F.levenshtein(F.col(\"RUT_AFILIADO\"), F.col(\"RUT\"))*100) / F.greatest(F.length(F.col(\"RUT_AFILIADO\")), F.length(F.col(\"RUT\"))), 0).cast(IntegerType()) )\n",
    "    print('\\tSimilitud Rut              [COMPLETADA]')\n",
    "\n",
    "    # ...FILTRO DE SIMILITUD...\n",
    "    df = df.filter(F.col(\"SIMILITUD\") >= 60) \n",
    "    \n",
    "    print( str(datetime.now().time()) + ' - Similitud Terminanda con digito verificador. ' + str(var))\n",
    "        \n",
    "    ################################################################################################################\n",
    "    ##                      Armado de Salida Por Dígito Verificador y Guardado en Bucket                          ##\n",
    "    ################################################################################################################\n",
    "    \n",
    "    print( str(datetime.now().time()) + ' - Rellenando y Uniendo Columnas faltantes.      ' + str(var))\n",
    "        \n",
    "    #...Agregamos alias a dataframe para simular consultas como si fuera base de datos. \n",
    "    #.. A: Dataframe de similitudes  -  B: Dataframe de TMP_REZAGO  -  C: Dataframe de TMP_AFILIADO\n",
    "    df = df.alias('A') \n",
    "    df_tmp_rezago = df_tmp_rezago.alias('B')\n",
    "    df_c = df_c.alias('C')\n",
    "    \n",
    "    #..Se realiza merge de tablas.\n",
    "    df = df.join(df_c, F.col('A.RUT') == F.col('C.RUT'), how='left').drop(F.col('C.RUT'))\n",
    "    df = df.join(df_tmp_rezago, F.col('A.ID') == F.col('B.ID'), how='left').drop(F.col('B.RUT_AFILIADO')).drop(F.col('B.APELLIDO_PATERNO'))\n",
    " \n",
    "    #...Definimos formatos de salida para columnas NECESARIAS.\n",
    "    df = df.withColumn('RUT', F.col('A.RUT').cast(IntegerType()))\n",
    "    df = df.withColumn('ROL_AFILIADO', F.lit(000))\n",
    "    df = df.withColumn('NACIONALIDAD', F.lit(' '))\n",
    "    df = df.withColumn('CORREO_ELECTRONICO', F.lit(' '))\n",
    "    df = df.withColumn('TELEFONO', F.lit(' '))\n",
    "    df = df.withColumn('ESTADO_CASO', F.lit(' '))\n",
    "    df = df.withColumn('FECHA_CREACION_CASO', F.lit(' '))\n",
    "    df = df.withColumn('FECHA_ACTUALIZACION_CASO', F.lit(' '))\n",
    "    df = df.withColumn('DOCUMENTACION', F.lit(' '))\n",
    "    df = df.withColumn('STATUS', F.lit(' '))\n",
    "    df = df.withColumn('RUT_EMPLEADOR_PAGO', F.lit(' '))\n",
    "    df = df.withColumn('CRITERIO_DE_ACLARACION', F.lit(' '))\n",
    "    df = df.withColumn('FECHA_RESOLUCION', F.lit(' '))\n",
    "    df = df.withColumn('RUT_AFILIADO', F.regexp_replace(F.col('A.RUT_AFILIADO'),\"''\",'0')).drop(F.col('A.RUT_AFILIADO'))\n",
    "    \n",
    "    #...Creación de salida con estructura final.\n",
    "    df = df.select(\n",
    "        'RUT',\n",
    "        'A.DV',\n",
    "        'ROL_AFILIADO',\n",
    "        'A.NOMBRES',\n",
    "        'A.AP_PATERNO',\n",
    "        'A.AP_MATERNO',\n",
    "        'C.PERIODO',\n",
    "        'NACIONALIDAD',\n",
    "        'C.SEXO',\n",
    "        'C.FECHA_NACIMIENTO',\n",
    "        'CORREO_ELECTRONICO',\n",
    "        'TELEFONO',\n",
    "        'ESTADO_CASO',\n",
    "        'FECHA_CREACION_CASO',\n",
    "        'FECHA_ACTUALIZACION_CASO',\n",
    "        'DOCUMENTACION',\n",
    "        'C.COD_INSTITUCION',\n",
    "        'C.FECHA_INGRESO_SISTEMA',\n",
    "        'C.FECHA_INCORPORACION_AFP',\n",
    "        'C.SITUACION_AFILIADO',\n",
    "        'A.ID',\n",
    "        'STATUS',\n",
    "        'RUT_EMPLEADOR_PAGO',\n",
    "        'CRITERIO_DE_ACLARACION',\n",
    "        'FECHA_RESOLUCION',\n",
    "        'B.AFP',\n",
    "        'B.RUT_EMPLEADOR',\n",
    "        'B.DV_EMPLEADOR',\n",
    "        'B.RAZON_SOCIAL',\n",
    "        'B.MAIL_EMPLEADOR',        \n",
    "        'B.DOMICILIO_EMPLEADOR',\n",
    "        'B.COMUNA_EMPLEADOR',\n",
    "        'RUT_AFILIADO',\n",
    "        'A.DV_AFILIADO',\n",
    "        'A.APELLIDO_PATERNO',\n",
    "        'A.APELLIDO_MATERNO',\n",
    "        F.col('A.NOM_TRABAJADOR').alias('NOMBRES_TRABAJADOR'),\n",
    "        'B.TIPO_PRODUCTO',\n",
    "        'B.P_DEVENGADO',\n",
    "        'B.REM_IMPONIBLE',\n",
    "        'B.MTO_COTIZACION',\n",
    "        'B.FECHA_PAGO',\n",
    "        'B.LLAVE_INTERNA_AFP',\n",
    "        F.col('A.NOM_TRABAJADOR').alias('NOMBRE_REZAGO'),\n",
    "        F.col('A.NOMBRES').alias('NOMBRE_AFILIADO'),\n",
    "        'EXISTENCIA',\n",
    "        'SIMILITUD',\n",
    "        'SIMILITUD_SIN_APMA',\n",
    "        'SIMILITUD_RUT_AFILIADO',\n",
    "        'SIMILITUDNOMBRE',\n",
    "        'SIMILITUDAPEPATERNO',\n",
    "        'SIMILITUDAPEMATERNO',\n",
    "        'SIMILITUDNOMBREREZAGOAPP',\n",
    "        'SIMILITUDNOMBREREZAGOAPM',\n",
    "        'SIMILITUDAPPATREZAGONOMBRE', \n",
    "        'SIMILITUDAPPATREZAGOAPMAT', \n",
    "        'SIMILITUDAPMATREZAGONOMBRE',\n",
    "        'SIMILITUDAPMATREZAGOAPPAT'\n",
    "    )\n",
    "    \n",
    "    ###\n",
    "    #df.createOrReplaceTempView('TMP')\n",
    "    #spark.sql('SELECT COUNT(*) FROM TMP').show()\n",
    "    ###\n",
    "    \n",
    "    df = df.repartition(10)\n",
    "    df.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "\n",
    "    print( str(datetime.now().time()) + ' - Escribiendo en bucket con digito verificador  ' + str(var))\n",
    "    \n",
    "    start = time.time()\n",
    "    df.write.format(\"parquet\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(F\"oci://previred_rezagos_files@ax1trrrsni7e/previred_rezagos_files/proceso_rezago_3_0/dv_{var}\")\n",
    "    end = time.time()\n",
    "    print(f'Tiempo guardando en bucket con dígito verificador {var}: ', end - start, '\\n')\n",
    "\n",
    "end = time.time()\n",
    "tiempo_transcurrido = end - start_master\n",
    "print(f'El Proceso Demoro ', int(tiempo_transcurrido // 60), ' Minutos y ' , int(tiempo_transcurrido % 60), ' Segundos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "###############################################################################################################\n",
    "##          Unimos PARQUETs y Guardamos 10 Particiones Unificadas Para Generar Tabla Externa                 ##\n",
    "###############################################################################################################\n",
    "start = time.time()\n",
    "\n",
    "# Le damos la direccion de todos los parquet dentro del bucket\n",
    "file_paths = [\"oci://previred_rezagos_files@ax1trrrsni7e/previred_rezagos_files/proceso_rezago_3_0/dv_*\"]\n",
    "\n",
    "dfs = [spark.read.parquet(file_path) for file_path in file_paths]\n",
    "\n",
    "# Se unifica a 1 solo data frame\n",
    "merged_df = dfs[0].unionAll(dfs[1:]) if len(dfs) > 1 else dfs[0]\n",
    "\n",
    "# se fuera que existan solo 10 reparticiones.\n",
    "merged_df = merged_df.repartition(10)\n",
    "\n",
    "# Se crea el nuevo input que sera la tabla externa al autonomos.\n",
    "merged_df.write.format(\"parquet\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"oci://previred_rezagos_files@ax1trrrsni7e/previred_rezagos_files/proceso_rezago_3_0/Salida_Final\")\n",
    "\n",
    "end = time.time()\n",
    "print(f'Tiempo guardando en bucket: ', end - start, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...Importante stopear la sesión de spark. Para CLOUD ejecutar '%stop_session'\n",
    "%stop_session\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v3v2_0]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v3v2_0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
